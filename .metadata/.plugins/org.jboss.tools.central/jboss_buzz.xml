<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>OpenSSL 3.0: Dealing with a Turkish locale bug</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/15/openssl-30-dealing-turkish-locale-bug" /><author><name>Dmitry Belyavskiy</name></author><id>3fa8d903-6ede-4e1d-a4d0-b70ad62334bc</id><updated>2022-06-15T07:00:00Z</updated><published>2022-06-15T07:00:00Z</published><summary type="html">&lt;p&gt;Changes in libraries have a risk of breaking things in unpredictable places. Debugging a crash is usually simple. But recently, in the late stages of working on the &lt;code&gt;curl&lt;/code&gt; utility for &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;Red Hat Enterprise Linux 9&lt;/a&gt;, I encountered a bug report that looked rather strange. In this article, I will discuss how this report led me to implement a localization bug fix during the late stages of RHEL 9 development.&lt;/p&gt; &lt;h2&gt;Why the curl utility with the Turkish locale failed&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;curl&lt;/code&gt; utility failed in a system configured with the Turkish locale (&lt;code&gt;tr_TR.utf8&lt;/code&gt;). The utility crashed in RHEL 9. It did not crash in Fedora 36; however, it failed to establish HTTPS connections.&lt;/p&gt; &lt;p&gt;It was easy to determine that the crash occurred when fetching one of the cipher algorithms—camellia-128. This was strange because the cipher was in the OpenSSL default provider, so it should be always available. The only suspicious thing was that the lookup name was in all caps: &lt;code&gt;CAMELLIA-128&lt;/code&gt;. However, OpenSSL makes a case-insensitive comparison for these fetches. The crash was related to a check that was absent in OpenSSL 3.0.1 (the base version in RHEL 9), which also explained the behavior difference between Fedora and RHEL.&lt;/p&gt; &lt;p&gt;Since the crash was locale-specific and presumably related to uppercase/lowercase comparison, I explored the case-insensitive comparison and the Turkish language variations as possible factors. Searching online yielded many resources, but this &lt;a href="http://www.i18nguy.com/unicode/turkish-i18n.html"&gt;article&lt;/a&gt; gives the best explanation. The alphabet used for English has a lowercase dotted i and an uppercase dotless I. But the Turkish alphabet has four letters for I: uppercase and lowercase dotted, and uppercase and lowercase dotless.&lt;/p&gt; &lt;p&gt;These additional Turkish letters change the relationship between the two letters English uses for I. Instead of the English case mapping of the lower dotted i to the upper dotless I, Turkish maps the lower dotted i to the upper dotted İ, and the lower dotless ı to the upper dotless I.&lt;/p&gt; &lt;p&gt;As it turns out, the change in the case rules for the letter &lt;em&gt;i&lt;/em&gt; frequently breaks software logic. As the article puts it, "Applications that have been internationalized are conditioned to easily accept new characters, collations, case rules, encodings, and other character-based properties and relationships. However, their design often does not anticipate that the properties or rules of English letters will change."&lt;/p&gt; &lt;p&gt;Armed with this knowledge, I set out to debug the utility for the Turkish locale.&lt;/p&gt; &lt;h2&gt;Match fails due to Turkish translations&lt;/h2&gt; &lt;p&gt;In a case-insensitive comparison in the Turkish locale, the uppercase name &lt;code&gt;CAMELLIA&lt;/code&gt; used for the lookup becomes lowercase with no dot: &lt;code&gt;camellıa.&lt;/code&gt; Since camellıa does not match the expected string (&lt;code&gt;camellia&lt;/code&gt;), the algorithm cannot be not found. Therefore, the omitted NULL check explains the crash.&lt;/p&gt; &lt;p&gt;The comparison uses &lt;code&gt;strcasecmp&lt;/code&gt; and &lt;code&gt;strncasecmp&lt;/code&gt; functions, which are locale-dependent. The &lt;code&gt;setlocale&lt;/code&gt; function will trigger the problem, but the command-line OpenSSL utility does not invoke that function, so the problem doesn't arise in the upstream test suite. &lt;code&gt;curl&lt;/code&gt; uses the system locale for this purpose. This issue did not occur in earlier versions of OpenSSL because it did not use a case-insensitive string comparison before version 3. Now, new versions use case-insensitive string comparisons more extensively.&lt;/p&gt; &lt;p&gt;So why was the connection not established even when the system did not crash? A debugging session revealed that the cause was another change between OpenSSL 1.1.1 and 3.0. The culprit was the so-called encoders/decoders mechanism for finding a relevant parser of ASN.1 structures. Encoders and decoders also identify objects by name. This time, there was a mismatch between &lt;code&gt;SubjectPublicKeyInfo&lt;/code&gt; and &lt;code&gt;subjectpublickeyinfo&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;If we want case insensitive comparison, we must always compare strings using a locale with predictable properties. Since algorithm names will never be internationalized, we can stop using locale-dependent &lt;code&gt;strcasecmp&lt;/code&gt; and &lt;code&gt;strncasecmp&lt;/code&gt; functions. We could compare strings byte-to-byte, but the profiling showed a significant performance slowdown. Luckily, POSIX implements a &lt;code&gt;strcasecmp_l&lt;/code&gt; function that accepts a specific locale object. If we pass a locale object matching the C locale to that function, we get the correct results.&lt;/p&gt; &lt;h3&gt;Adopt a debugging process early&lt;/h3&gt; &lt;p&gt;In theory, the rest of the process is simple: initialize a global object matching the C locale; use it everywhere in the OpenSSL code; free on shutdown. However, this leads to problems with the OpenSSL Federal Information Processing Standards (FIPS) provider. By design, FIPS requirements should not rely on a global object external to the module itself. This means we have to create another object inside the provider and manage it locally.&lt;/p&gt; &lt;p&gt;The resulting patch is one of the most invasive I have ever written. It changes approximately 80 files in the OpenSSL source tree and required serious testing. Global find-and-replace completes most of the changes. But some of them affect the core functionality of OpenSSL and, as mentioned before, touch the &lt;a href="https://developers.redhat.com/articles/2022/05/31/your-go-application-fips-compliant"&gt;FIPS module&lt;/a&gt;. In order for it to be accepted upstream, we had to build it to accommodate non-Linux platforms as well, and that required additional changes. We had a long discussion with the upstream developers on how to include the patch in the stable 3.0 series&lt;/p&gt; &lt;p&gt;During the course of preparing this article for publication, there were long discussions within upstream and the community. As a result, upstream got rid of the locale object and switched the implementation to byte-to-byte comparison. We are going to switch our implementation in the same way, but that won't happen soon.&lt;/p&gt; &lt;p&gt;You may come across internationalization-related issues everywhere, even if you think that you rely on plain ASCII. A wide client base and early adoption could help you find a particular issue early on when it is significantly less burdensome.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/15/openssl-30-dealing-turkish-locale-bug" title="OpenSSL 3.0: Dealing with a Turkish locale bug"&gt;OpenSSL 3.0: Dealing with a Turkish locale bug&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Dmitry Belyavskiy</dc:creator><dc:date>2022-06-15T07:00:00Z</dc:date></entry><entry><title type="html">Infinispan 14 indexing &amp;#38; query news</title><link rel="alternate" href="https://infinispan.org/blog/2022/06/14/infinispan-14-indexing-query-news" /><author><name>Fabio Massimo Ercoli</name></author><id>https://infinispan.org/blog/2022/06/14/infinispan-14-indexing-query-news</id><updated>2022-06-14T18:00:00Z</updated><content type="html">Dear Infinispan community, with the Infinispan 14 development release 03 we introduced some news on indexing and search capabilities. INFINISPAN INDEXING ANNOTATIONS Hibernate annotations are going to be replaced with the new Infinispan indexing annotations, that will be used in the same exact way for both embedded and remote queries. Here is an example of two annotated POJOs: Poem.java @Indexed public class Poem { private Author author; private String description; private Integer year; @Embedded(includeDepth = 2, structure = Structure.NESTED) public Author getAuthor() { return author; } @Text(projectable = true, analyzer = "whitespace", termVector = TermVector.WITH_OFFSETS) public String getDescription() { return description; } @Basic(projectable = true, sortable = true, indexNullAs = "1800") public Integer getYear() { return year; } } Author.java @Indexed public class Author { private String name; public Author(String name) { this.name = name; } @Keyword(projectable = true, sortable = true, normalizer = "lowercase", indexNullAs = "unnamed", norms = false) public String getName() { return name; } public void setName(String name) { this.name = name; } } Indexed fields without any special string/text transformation will be annotated as @Basic. If we need to apply a normalizer to a String field, we will opt for a @Keyword annotation. If we need to apply an analyzer to a String field, we will opt for a @Text annotation. The new annotations allow setting with the same annotation if the field should be sortable, or projectable, or its normalizer or its analyzer. However, not all the combinations will be possible, for instance the attribute sortable is not present on the @Text annotation, since an analyzed field cannot be used to sort the result set. indexNullAs attribute allow now to define a default value to use on index in case the corresponding entity values was null. Embedded indexes are defined using the @Embedded annotation, and it is possible to choose between the NESTED structure which preserves the original object relationship structure and FLATTENED structure which makes the leaf fields multi-valued fields of the parent entity. INDEX STARTUP MODE Sometimes indexes can be persistent and cache data not or vice versa, for those cases it can be useful to perform some operations to ensure the index will be consistent with data in the cache. We introduced the startup-mode configuration. Here is an example: &lt;distributed-cache&gt; &lt;indexing storage="filesystem" startup-mode="purge"&gt; &lt;!-- Additional indexing configuration goes here. --&gt; &lt;/indexing&gt; &lt;/distributed-cache&gt; With this configuration every time the cache is started, the indexes will be purged. Possible values are: purge, reindex, none and auto, with the latter Infinispan will decide if and in case which operation to perform according to how indexes ad cache data are configured. INDEX SCHEMA UPDATE This should be considered an advanced feature to be used only in case your model needs to be evolved time to time, and you need to continue to query your cache data without the aid of some data migrations or reindexing. For a comprehensive guide about when to use schema update instead of migrate or reindex the data please refer to the documentation. The command can be triggered from the HotRod remote administration API: remoteCacheManager.administration().updateIndexSchema(CACHE_NAME); or using the REST API, targeting the uri: POST .../v2/caches/{cacheName}/search/indexes?action=updateSchema or using the Infinispan cli by running update-schema on the runtime cache instance.</content><dc:creator>Fabio Massimo Ercoli</dc:creator></entry><entry><title>4 tips for achieving better security on Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/14/4-tips-achieving-better-security-kubernetes" /><author><name>Ajmal Kohgadai, Andy Oram</name></author><id>bbc59e5a-2379-48a8-8fa7-a93f4257cc56</id><updated>2022-06-14T07:00:00Z</updated><published>2022-06-14T07:00:00Z</published><summary type="html">&lt;p&gt;When security is ignored, organizations are putting at risk the core benefit of faster application development and releases. But security and agility do not have to be in contention.&lt;/p&gt; &lt;p&gt;A recent Red Hat survey with more than 300 respondents, covered in our &lt;a href="https://www.redhat.com/en/resources/state-kubernetes-security-report"&gt;2022 State of Kubernetes security report&lt;/a&gt;, identified the most pressing security needs and offered suggestions for putting your organization on track to protect security in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; environments.&lt;/p&gt; &lt;p&gt;Our findings show that what happens in the build and deploy stages has a significant impact on security, as revealed by the prevalence of misconfigurations and vulnerabilities across organizations. Security, therefore, must shift left, embedded imperceptibly into &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; workflows instead of being "bolted on" when the application is about to be deployed into production.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Most of the material in this article comes from the Red Hat report, &lt;a href="https://www.redhat.com/en/resources/state-kubernetes-security-report"&gt;2022 State of Kubernetes security&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Top Kubernetes security tasks identified in the survey&lt;/h2&gt; &lt;p&gt;Organizations expect a security solution that protects containers and Kubernetes at every phase. Security starts at the earliest phases of development, such as &lt;a href="https://developers.redhat.com/articles/2022/03/07/manage-python-security-thoths-cloud-based-dependency-resolver"&gt;choosing secure third-party libraries&lt;/a&gt;, and extends to runtime monitoring and detection.&lt;/p&gt; &lt;p&gt;The methods used to protect applications span DevOps and security activities, underscoring the need for both in a &lt;a href="https://developers.redhat.com/topics/devsecops"&gt;DevSecOps&lt;/a&gt; approach in which the development, operations, and security teams collaborate.&lt;/p&gt; &lt;p&gt;The most important security tasks that respondents identified as "must-have" capabilities in Kubernetes (Figure 1) were:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runtime threat detection and response (chosen by 69% of respondents)&lt;/li&gt; &lt;li&gt;Configuration management (chosen by 68% of respondents)&lt;/li&gt; &lt;li&gt;Image scanning and vulnerability management (chosen by 65% of respondents)&lt;/li&gt; &lt;/ul&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/top%20security%20tasks%20in%20kubernetes.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/top%20security%20tasks%20in%20kubernetes.png?itok=3WbCcPzo" width="1338" height="1016" alt="A cluster of three or four concerns topped the list of needed security capabilities." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A cluster of three or four concerns topped the list of needed security capabilities. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;When the respondents refer to configuration management, they are pointing out that security must currently be dropped into many different properties of YAML files with a variety of isolated parameters that are hard to learn and coordinate.&lt;/p&gt; &lt;h2&gt;Tips for starting on a stronger security course&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://www.redhat.com/en/resources/state-kubernetes-security-report"&gt;2022 State of Kubernetes security report&lt;/a&gt; ended with the following suggestions.&lt;/p&gt; &lt;h3&gt;Use Kubernetes-native security architectures and controls&lt;/h3&gt; &lt;p&gt;Kubernetes-native security uses the rich declarative data and native controls in Kubernetes to deliver several key security benefits. Analyzing the declarative data available in Kubernetes yields better security, with risk-based insights into configuration management, compliance, segmentation, and Kubernetes-specific vulnerabilities.&lt;/p&gt; &lt;p&gt;Using the same infrastructure and its controls for application development and security reduces the learning curve and supports faster analysis and troubleshooting. This consistent infrastructure also eliminates operational conflict by granting security the same automation and scalability advantages that Kubernetes extends to infrastructure.&lt;/p&gt; &lt;h3&gt;Start security early, but extend it across the full life cycle&lt;/h3&gt; &lt;p&gt;Security has long been viewed as a business inhibitor, especially by developers and DevOps teams whose core mandates are to deliver code fast. With containers and Kubernetes, security should become a business accelerator, by helping developers build strong security into their assets right from the start.&lt;/p&gt; &lt;p&gt;Look for a container and Kubernetes security platform that incorporates DevOps best practices and internal controls as part of its configuration checks. Such a platform should also employ tools that assess the security posture of the Kubernetes configuration itself, so that developers can focus on feature delivery. The open source &lt;a href="https://github.com/stackrox/kube-linter"&gt;KubeLinter&lt;/a&gt; tool picks up many security issues in Kubernetes configurations.&lt;/p&gt; &lt;h3&gt;Require portability across hybrid environments&lt;/h3&gt; &lt;p&gt;With most organizations deploying containers in both on-premises and public cloud environments (sometimes in multiple clouds), security must apply consistently wherever your assets are running. The common foundation is Kubernetes, so make Kubernetes your source of truth, your point of enforcement, and your universal visibility layer for consistent security. Managed Kubernetes services can quicken your organization’s adoption of Kubernetes, but be careful about getting locked into cloud provider-specific tooling and services.&lt;/p&gt; &lt;h3&gt;Transform the developer into a security user by building a bridge between DevOps and security&lt;/h3&gt; &lt;p&gt;Given that most organizations expect DevOps to run container security platforms, your security tooling must help bridge security and DevOps. To be effective, the platform must have security controls that make sense in a containerized, Kubernetes-based environment. It should also assess risk appropriately. Telling a developer to fix all discovered vulnerabilities that have a Common Vulnerability Scoring System (CVSS) score of 7 or higher is inefficient. Instead, you can improve security significantly by identifying the three deployments that are exposed to the most severe vulnerabilities and showing the developers why these deployments are at risk.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Learning the best practices outlined in this article is a good first step toward improving your application's security. The speed with which new containerized applications are built and deployed as &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; requires security automation in developer workflows. Developers shouldn’t have to slow down to run manual security checks. Organizations should deploy automated security in a DevOps fashion, using security tools that enable an open hybrid strategy for application security. Learn more about how &lt;a href="https://www.redhat.com/en/resources/advanced-cluster-security-for-kubernetes-datasheet"&gt;Red Hat Advanced Cluster Security for Kubernetes&lt;/a&gt; can provide developers with the security guardrails to help them automate DevSecOps in their pipelines.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/14/4-tips-achieving-better-security-kubernetes" title="4 tips for achieving better security on Kubernetes"&gt;4 tips for achieving better security on Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ajmal Kohgadai, Andy Oram</dc:creator><dc:date>2022-06-14T07:00:00Z</dc:date></entry><entry><title>Use OpenVINO to convert speech to text</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/13/use-openvino-convert-speech-text" /><author><name>Sean Pryor</name></author><id>2dc726d3-1f06-4051-b51f-115f84829bcf</id><updated>2022-06-13T17:48:00Z</updated><published>2022-06-13T17:48:00Z</published><summary type="html">&lt;p&gt;Speech to text is one of the most common use cases for artificial intelligence. It's used all over to allow easier human interaction. Phone tree automation is a common use case.&lt;/p&gt; &lt;p&gt;This article will walk you through a speech-to-text example using OpenVINO, an open-source toolkit for optimizing and deploying AI inference. This example is a variant of the OpenVINO speech-to-text demo notebook which can be found in &lt;a href="https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/211-speech-to-text/211-speech-to-text.ipynb"&gt;OpenVINO's GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;What is QuartzNet?&lt;/h2&gt; &lt;p&gt;QuartzNet is a variant of a Jasper network that performs speech-to-text translation.&lt;/p&gt; &lt;p&gt;The inputs to the network are a series of units called &lt;em&gt;mel spectrograms.&lt;/em&gt; These are a way of representing audio data that involves several steps of processing.&lt;/p&gt; &lt;p&gt;First, the raw audio signal is divided into overlapping sections, and then a Fourier transformation is applied to them converting from signals over time to frequencies. Then the log scale of the frequency is compared to the amplitude to form a spectrogram.&lt;/p&gt; &lt;p&gt;Finally, the spectrogram's domain is changed to the &lt;a href="https://www.sfu.ca/sonic-studio-webdav/handbook/Mel.html"&gt;mel scale&lt;/a&gt;, which is a frequency scale that better differentiates between the ranges of frequency that human speech and hearing cover, forming a mel spectrogram.&lt;/p&gt; &lt;p&gt;For more on mel spectrograms, read &lt;a href="https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53"&gt;Leland Roberts' article on the subject&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;OpenVINO toolkit&lt;/h2&gt; &lt;p&gt;OpenVINO is a framework for optimizing models, as well as an optimized inference server. It allows you to perform several optimizations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Quantization&lt;/strong&gt;: Reducing floating point precision to increase processing speed. INT8 can be orders of magnitude faster than FP16 with similar levels of precision in some cases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accuracy-aware quantization&lt;/strong&gt;: Automated quantization that preserves a user-specified level of accuracy.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pruning and sparsity&lt;/strong&gt;: Reducing unnecessary complexity of the model. For example, this could involve removing layers that aren't contributing much to the overall result or weights that are extremely small.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Operation fusing&lt;/strong&gt;: Combining several model layers into one. This gives equivalent accuracy but can run significantly faster on Intel hardware given the use of specialized instructions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;On &lt;a href="https://developers.redhat.com/products/red-hat-openshift-data-science/overview"&gt;Red Hat OpenShift Data Science&lt;/a&gt;, the default deployment is done on Intel hardware, meaning there is no additional setup required.&lt;/p&gt; &lt;p&gt;The notebook we'll be looking at in this article covers downloading a QuartzNet model, converting it to OpenVINO Intermediate Representation (IR), serving it via OpenVINO Model Server, sending mel spectrograms of English-language audio for inference, and decoding the results using a simple algorithm. Note that this is an example; some parts, such as the decoding algorithm, could be improved if one were to adapt this for a production use case.&lt;/p&gt; &lt;p&gt;The OpenVINO Model Server (OVMS) is an Intel-optimized model server, which allows a user to serve multiple models, keep track of generations of models, and lets users update them without downtime.&lt;/p&gt; &lt;h2&gt;Download the QuartzNet model&lt;/h2&gt; &lt;p&gt;OpenVINO has its &lt;a href="https://github.com/openvinotoolkit/open_model_zoo"&gt;own model zoo&lt;/a&gt; where you can browse and download pre-compiled and pre-trained models from.&lt;/p&gt; &lt;p&gt;For this demo, we will download an ONNX-format QuartzNet model. The ONNX (Open Neural Network Exchange Format) format is easily portable for exchanging models. It allows a user to package a model from a range of frameworks easily into a single file, and is easy to reinstantiate from that single file allowing for great portability. OVMS will later convert this format into its own IR for optimization.&lt;/p&gt; &lt;p&gt;The notebook, we first start with a few bits of boilerplate by setting up the paths:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; model_folder = "model" download_folder = "output" data_folder = "data" precision = "FP16" model_name = "quartznet-15x5-en" &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;&lt;code&gt;omz_downloader&lt;/code&gt; automatically creates a directory structure and downloads the selected model. This step is skipped if the model is already downloaded. The selected model comes from the public directory, which means it must be converted into Intermediate Representation (IR).&lt;/p&gt; &lt;pre&gt; &lt;code&gt; # Check if model is already downloaded in download directory path_to_model_weights = Path(f'{download_folder}/public/{model_name}/models') downloaded_model_file = list(path_to_model_weights.glob('*.pth')) if not path_to_model_weights.is_dir() or len(downloaded_model_file) == 0: download_command = f"omz_downloader --name {model_name} --output_dir {download_folder} --precision {precision}" ! $download_command &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;Convert the model to IR&lt;/h2&gt; &lt;p&gt;Next, we need to convert the model from ONNX format into OpenVINO IR format, which consists of three files.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;XML:&lt;/strong&gt; The XML file describes the layers of the network, their dimensions, and parameters. It also describes the data flow. However, it does not store the actual weights; those are instead references to the bin file, which contains the weights and other large values.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bin: &lt;/strong&gt;This file contains the large constant values like layer weights and other things that detail the state of the model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mapping:&lt;/strong&gt; This file contains some additional metadata detailing things like the IO between layers.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A more detailed explanation is available in the &lt;a href="https://docs.openvino.ai/latest/openvino_docs_MO_DG_IR_and_opsets.html"&gt;OpenVINO docs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;omz_converter&lt;/code&gt; converts the pre-trained PyTorch model to the ONNX model format, which is further converted to the OpenVINO IR format. Both stages of conversion are handled by calling &lt;code&gt;omz_converter&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; # Check if model is already converted in model directory path_to_converted_weights = Path(f'{model_folder}/public/{model_name}/{precision}/{model_name}.bin') if not path_to_converted_weights.is_file(): convert_command = f"omz_converter --name {model_name} --precisions {precision} --download_dir {download_folder} --output_dir {model_folder}" ! $convert_command &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;In the end, you should have the following files:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;quartznet-15x5-en.bin&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;quartznet-15x5-en.mapping&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;quartznet-15x5-en.xml&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Upload the model and directory structure to S3&lt;/h2&gt; &lt;p&gt;In OVMS, the model server looks for the following directory structure:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; tree models/ models/ ├── model1 │ ├── 1 │ │ ├── ir_model.bin │ │ └── ir_model.xml │ └── 2 │ ├── ir_model.bin │ └── ir_model.xml └── model2 │ └── 1 │ ├── ir_model.bin │ ├── ir_model.xml │ └── mapping_config.json └── model3 └── 1 └── model.onnx &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You can find more about the model repository directory structure in the &lt;a href="https://docs.openvino.ai/latest/ovms_docs_models_repository.html"&gt;OpenVINO docs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The next step is uploading the OpenVINO IR files to S3. This demo assumes that you have a local S3 set up in advance; setting that up is beyond the scope of this demo. If you don't have access to a real S3 bucket, there are alternatives like Ceph RadosGW or Google Storage, which are also supported by OVMS.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; import boto3 access_key = 'S3_ACCESS_KEY' # &lt;- Replace with actual key secret_key = 'S3_SECRET_KEY' # &lt;- Replace with actual key s3 = boto3.client('s3', endpoint_url='ENDPOINT_URL', # &lt;- This is only necessary when using Ceph RadosGW aws_access_key_id=access_key, aws_secret_access_key=secret_key,) s3.upload_file('model/public/quartznet-15x5-en/FP16/quartznet-15x5-en.bin', 'openvino-quartznet', '1/quartznet-15x5-en.bin') s3.upload_file('model/public/quartznet-15x5-en/FP16/quartznet-15x5-en.mapping', 'openvino-quartznet', '1/quartznet-15x5-en.mapping') s3.upload_file('model/public/quartznet-15x5-en/FP16/quartznet-15x5-en.xml', 'openvino-quartznet', '1/quartznet-15x5-en.xml') &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This gives us the following structure in s3:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; # tree s3://openvino-quartznet/ └── 1 ├── quartznet-15x5-en.bin ├── quartznet-15x5-en.mapping └── quartznet-15x5-en.xml &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;Create an OVMS instance&lt;/h2&gt; &lt;p&gt;Now that we have uploaded the model to S3, we can create an instance of the OpenVINO Model Server to serve the model. Intel has an OVMS Operator that will allow users to easily provision an OVMS instance. Here's an example custom resource for the Operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; kind: ModelServer apiVersion: intel.com/v1alpha1 metadata: name: openvino-quartznet-model-server namespace: your-project-namespace spec: image_name: &gt;- registry.connect.redhat.com/intel/openvino-model-server@sha256:f670aa3dc014b8786e554b8a3bb7e2e8475744d588e5e72d554660b74430a8c5 deployment_parameters: replicas: 1 resources: limits: cpu: '4' memory: '4Gi' requests: cpu: '4' memory: '4Gi' service_parameters: grpc_port: 8080 rest_port: 8081 models_settings: single_model_mode: true config_configmap_name: '' model_config: '' model_name: 'quartznet' # This is the name the model is served with model_path: 's3://openvino-quartznet/' # This URL path to where the model repository was stored earlier nireq: 0 plugin_config: '{"CPU_THROUGHPUT_STREAMS":1}' batch_size: '' shape: '(1, 64, 176)' # This is needed due to the notebook having a slightly different input shape than the default. OVMS handles this conversion automatically model_version_policy: '{"latest": { "num_versions":1 }}' layout: '' target_device: CPU is_stateful: false idle_sequence_cleanup: false low_latency_transformation: true max_sequence_number: 0 server_settings: file_system_poll_wait_seconds: 0 sequence_cleaner_poll_wait_minutes: 0 log_level: INFO grpc_workers: 1 rest_workers: 0 models_repository: storage_type: S3 https_proxy: '' http_proxy: '' models_host_path: '' models_volume_claim: '' aws_secret_access_key: 'S3_SECRET_KEY' # Replace with actual key aws_access_key_id: 'S3_ACCESS_KEY' # Replace with actual key aws_region: '' s3_compat_api_endpoint: 'ENDPOINT_URL' # This is only necessary when using Ceph RadosGW gcp_creds_secret_name: '' azure_storage_connection_string: '' &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Once the server finishes initializing, the model will be available on both gRPC and HTTP endpoints.&lt;/p&gt; &lt;h3&gt;ovmsclient&lt;/h3&gt; &lt;p&gt;For a simple, lightweight client, &lt;code&gt;ovmsclient&lt;/code&gt; is an easy way to interact with an OVMS server. The client maintains an underlying gRPC client to OVMS and provides several convenience features. For starters, the following code allows users to connect to and query the input and output parameters of the model:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; import ovmsclient import librosa import numpy as np import scipy client = ovmsclient.make_grpc_client("openvino-quartznet-model-server.default.svc.cluster.local:8080") model_metadata = client.get_model_metadata(model_name="quartznet") print(model_metadata) {'model_version': 1, 'inputs': {'audio_signal': {'shape': [1, 64, 176], 'dtype': 'DT_FLOAT'}}, 'outputs': {'output': {'shape': [1, 88, 29], 'dtype': 'DT_FLOAT'}}} &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;We can see that the shape is the input shape defined in the CR above.&lt;/p&gt; &lt;h3&gt;Convert audio data to mel&lt;/h3&gt; &lt;p&gt;In order to perform inference, raw audio data must be converted to the mel spectrograms we discussed above. The code below performs this conversion:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; # First load the audio data, in this case a clip of English audio with the speaker saying "from the edge to the cloud" audio, sampling_rate = librosa.load(path=f'data/edge_to_cloud.ogg', sr=16000) # This first function converts the audio to mel spectrograms. This has specific window sizing and a hardcoded sampling rate. A different algorithm could be implemented if user needs differ. def audio_to_mel(audio, sampling_rate): assert sampling_rate == 16000, "Only 16 KHz audio supported" preemph = 0.97 preemphased = np.concatenate([audio[:1], audio[1:] - preemph * audio[:-1].astype(np.float32)]) # Calculate window length win_length = round(sampling_rate * 0.02) # Based on previously calculated window length run short-time Fourier transform spec = np.abs(librosa.core.spectrum.stft(preemphased, n_fft=512, hop_length=round(sampling_rate * 0.01), win_length=win_length, center=True, window=scipy.signal.windows.hann(win_length), pad_mode='reflect')) # Create mel filter-bank, produce transformation matrix to project current values onto Mel-frequency bins mel_basis = librosa.filters.mel(sampling_rate, 512, n_mels=64, fmin=0.0, fmax=8000.0, htk=False) return mel_basis, spec # This function changes the mel spectrograms by converting them to a logarithmic scale, normalizing them, and adding padding to make processing easier. Note that this padding ensures the input shape is consistent, and matches the (1, 64, 176) we supplied as the input shape when creating the model server instance. def mel_to_input(mel_basis, spec, padding=16): # Convert to logarithmic scale log_melspectrum = np.log(np.dot(mel_basis, np.power(spec, 2)) + 2 ** -24) # Normalize output normalized = (log_melspectrum - log_melspectrum.mean(1)[:, None]) / (log_melspectrum.std(1)[:, None] + 1e-5) # Calculate padding remainder = normalized.shape[1] % padding if remainder != 0: return np.pad(normalized, ((0, 0), (0, padding - remainder)))[None] return normalized[None] mel_basis, spec = audio_to_mel(audio=audio.flatten(), sampling_rate=sampling_rate) audio = mel_to_input(mel_basis=mel_basis, spec=spec) # The inference server requires a dict that has the following formatting. The input key is the same 'audio_signal' that was returned by the metadata call above inputs = {'audio_signal': audio} # If we look at the shape with the included padding, it's the same shape as the model is expecting now print(audio.shape) (1, 64, 176) &lt;/code&gt; &lt;/pre&gt; &lt;h3&gt;Inference example&lt;/h3&gt; &lt;p&gt;The final step is to actually perform the inference. This involves using &lt;code&gt;ovmsclient&lt;/code&gt; to make the inference call, as well as decoding the results. As noted above, the decoding step in this example is a simpler example than would be expected in a production environment and is only provided for demo purposes. In particular, it only decodes each letter as it changes, meaning that words that require repeated letters wouldn't work. In our example, the words contain no double letters, and so will work fine, but please be aware of the example's limitations.&lt;/p&gt; &lt;p&gt;At the end, we'll have an iterator containing predictions for each time, and the index of each corresponding to a letter in the alphabet array.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; character_probabilities = client.predict(inputs = inputs, model_name="quartznet") alphabet = " abcdefghijklmnopqrstuvwxyz'~" # These correspond to the 29 different outputs, the value being the probability of each character. We take the maximum prediction as the highest probability for a given letter. character_probabilities = next(iter(character_probabilities)) # Remove unnecessary dimension (we are doing inference in batches of 1) character_probabilities = np.squeeze(character_probabilities) # Run argmax to pick most possible symbols character_probabilities = np.argmax(character_probabilities, axis=1) def ctc_greedy_decode(predictions): previous_letter_id = blank_id = len(alphabet) - 1 transcription = list() for letter_index in predictions: if previous_letter_id != letter_index != blank_id: transcription.append(alphabet[letter_index]) previous_letter_id = letter_index return ''.join(transcription) transcription = ctc_greedy_decode(character_probabilities) print(transcription) from the edge to the cloud &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, you've seen an end-to-end example setup for voice transcription using OpenVINO. This has many applications, from note taking to chatbots to voice search. Using OpenVINO, the model can easily be optimized for any target hardware footprint as well, allowing it to be used anywhere from the edge to the cloud.&lt;/p&gt; &lt;p&gt;For a deeper dive, check out the &lt;a href="https://github.com/Xaenalt/OpenVINO-DevCON-Speech-to-text"&gt;complete code for this notebook example&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/13/use-openvino-convert-speech-text" title="Use OpenVINO to convert speech to text"&gt;Use OpenVINO to convert speech to text&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Sean Pryor</dc:creator><dc:date>2022-06-13T17:48:00Z</dc:date></entry><entry><title type="html">Quick JBang Scripting With IntelliJ Idea</title><link rel="alternate" href="http://www.mastertheboss.com/java/quick-jbang-scripting-with-intellij/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/quick-jbang-scripting-with-intellij/</id><updated>2022-06-13T07:29:26Z</updated><content type="html">This quick article shows how to install and use JBang’s IntelliJ Idea plugin to create JBang projects and scripts in no time. Firstly, to get started quickly with JBang, we recommend having a look at this article: JBang: Create Java scripts like a pro Next, let’s see how to install JBang Plugin on IntelliJ Idea ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>A quick way to translate physical addresses into virtual ones</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/13/quick-way-translate-physical-addresses-virtual-ones" /><author><name>Noah Sanci</name></author><id>6f155b24-4a0a-4743-a319-ad07b799c29e</id><updated>2022-06-13T07:00:00Z</updated><published>2022-06-13T07:00:00Z</published><summary type="html">&lt;p&gt;Recently, I have been working on enabling cooperation between SystemTap (a kernel profiling tool) and gprof (a tool that makes graphs from program profiles). This exercise has given me insight into meaningful topics only briefly touched upon at my university, such as kernel space, user space, and virtual memory. But these concepts are fundamental to the proper and safe execution of programs on any modern operating system. The trade off is some address translation when viewing memory from kernel space versus user space. In this article, you'll see how that translation can be handled.&lt;/p&gt; &lt;h2&gt;A quick dive into user and kernel address spaces&lt;/h2&gt; &lt;p&gt;User space and kernel space are kept separate in all modern OSes. This separation is an essential layer of security between information essential to the operation of the machine and the applications on that machine.&lt;/p&gt; &lt;p&gt;As the term suggests, &lt;em&gt;user process address space&lt;/em&gt; is the area where user-run processes are stored and store information. &lt;em&gt;Text pages&lt;/em&gt; are a portion of the process address space that stores the program’s code.&lt;/p&gt; &lt;p&gt;Consider a simple calculator program, of the sort installed on most OSes. If you ran this calculator program with the expression &lt;code&gt;5+5&lt;/code&gt; and a web browser at the same time without any preventative measures, the browser might be able to access the mathematical expression located in the calculator’s address space and change it to a different expression, such as &lt;code&gt;6+6&lt;/code&gt;. Common sense dictates that this should not occur except under very unique circumstances. The operating system provides the program separation that the user desires by giving each process its own isolated address space.&lt;/p&gt; &lt;p&gt;The kernel address space is the realm of the kernel code and information that may be useful to other programs, such as user input/output and network data. Kernel space information is generally inaccessible except through functions known as &lt;em&gt;system calls.&lt;/em&gt;&lt;/p&gt; &lt;h2&gt;How virtual memory works&lt;/h2&gt; &lt;p&gt;&lt;em&gt;Virtual memory&lt;/em&gt; is a simple convenience in the form of an abstraction. Without virtual memory, applications would need to manage their physical memory space, coordinating with every other process running on the computer. Virtual memory leaves that management to the kernel by creating maps that allow translation between virtual and physical memory. Applications use virtual memory, and the kernel’s memory management unit (MMU) uses physical memory. The MMU is the mapping hardware that translates virtual memory addresses to physical memory addresses.&lt;/p&gt; &lt;p&gt;To understand how virtual memory works, suppose there are two running programs: a calculator with the expression &lt;code&gt;5+5&lt;/code&gt; and an internet browser. The calculator may have the expression &lt;code&gt;5+5&lt;/code&gt; stored in the address &lt;code&gt;0xdeadbeef&lt;/code&gt;, and the browser could store any of its data, such as an IP address, in &lt;code&gt;0xdeadbeef&lt;/code&gt; as well. Why doesn't one piece of information overwrite the other? The addresses where the programs are storing their data are located in virtual memory, and they refer to different locations in physical memory. Although the virtual addresses are the same, the OS handles the physical addresses, which are completely different.&lt;/p&gt; &lt;p&gt;The OS may also take a page from a process and write it to disk to make space for another process page, resulting in two operations at the same physical address.&lt;/p&gt; &lt;h2&gt;SystemTap and gprof cooperation&lt;/h2&gt; &lt;p&gt;I have been working on leveraging the data collected by SystemTap and outputting it into a gprof-readable format. SystemTap profiles programs that use shared libraries. The problem is that SystemTap can load those libraries anywhere in virtual memory. It must relativize their addresses and process them later.&lt;/p&gt; &lt;p&gt;&lt;code&gt;umodaddr()&lt;/code&gt;, shown in the listing below, was created for the purpose of this translation:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;function umodaddr:long (address:long) %{ /* pragma:vma */ long vm_start = -1; stap_find_vma_map_info(current, STAP_ARG_address, &amp;vm_start, NULL, NULL, NULL,NULL); if(vm_start == -1) STAP_ERROR("umodaddr 0x%llx unknown\n ", STAP_ARG_address); STAP_RETURN(STAP_ARG_address - vm_start); %} &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This function takes the virtual address of the called function, found in &lt;code&gt;STAP_ARG_address,&lt;/code&gt; and returns its offset from the beginning of the virtual memory. This function becomes a tool to ensure that SystemTap outputs addresses relative to the beginning of their virtual memory, enabling compatibility with gprof.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;With this translation complete, a SystemTap script can use these translated addresses to create a file that can be interpreted by gprof. This file will be of the gmon.out format, so it will internally contain a histogram. A script can then automatically invoke gprof on all of these files, giving us the ability to leverage gprof's functionality to inspect a process's activity.&lt;/p&gt; &lt;p&gt;Understanding how virtual memory works made it possible to combine the extremely low-level capabilities of SystemTap with the informative functionality of gprof. This will allow users to gain insight into where precisely a program spends time and contribute to solving time-sensitive issues.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/13/quick-way-translate-physical-addresses-virtual-ones" title="A quick way to translate physical addresses into virtual ones"&gt;A quick way to translate physical addresses into virtual ones&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Noah Sanci</dc:creator><dc:date>2022-06-13T07:00:00Z</dc:date></entry><entry><title type="html">How to validate Jakarta REST parameters</title><link rel="alternate" href="http://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-validate-jakarta-rest-endpoint-values/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-validate-jakarta-rest-endpoint-values/</id><updated>2022-06-10T16:08:56Z</updated><content type="html">This article is a walk through the available options to perform validation of REST Endpoint parameters using Jakarta EE. We will show both Bean validation and Entity validation of a REST Service endpoint. The Jakarta Bean Validation specification allows to define some validation rules, from some built-in on a single field to complex customer rules. ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Get started with Red Hat OpenShift Connectors</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/09/get-started-red-hat-openshift-connectors" /><author><name>Bernard Tison</name></author><id>063945e9-6a30-43ef-8a28-d6f112c9bfec</id><updated>2022-06-09T10:45:00Z</updated><published>2022-06-09T10:45:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/connectors"&gt;Red Hat OpenShift Connectors&lt;/a&gt; is a new cloud service offering from Red Hat. They are pre-built connectors for quick and reliable connectivity across data, services, and systems. Connectors are delivered as a fully managed service, tightly integrated with &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;, Red Hat's managed cloud service for Apache Kafka.&lt;/p&gt; &lt;p&gt;Red Hat OpenShift Connectors is in the Service Preview phase at the moment. As part of the Service Preview program, you can deploy up to four connectors free of charge. The connectors are deleted after 48 hours.&lt;/p&gt; &lt;p&gt;We make a distinction between &lt;em&gt;source&lt;/em&gt; and &lt;em&gt;sink&lt;/em&gt; connectors. A source connector allows you to send data from an external system to OpenShift Streams for Apache Kafka. A sink connector allows you to send data from OpenShift Streams for Apache Kafka to an external system.&lt;/p&gt; &lt;p&gt;At the moment we offer more than 50 source and sink connectors. These include source and sink connectors to a variety of cloud services based on the awesome &lt;a href="https://developers.redhat.com/topics/camel-k"&gt;Camel K&lt;/a&gt; technology, as well as source connectors for databases based on the popular &lt;a href="https://debezium.io"&gt;Debezium&lt;/a&gt; project that performs change data capture. The complete list of available connectors to date can be found at the &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/connectors"&gt;Red Hat OpenShift Connectors site&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article shows you how to get started with OpenShift Connectors. You will learn how to create a source connector and a sink connector and send data to and from topics in OpenShift Streams for Apache Kafka.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;This article assumes that you have created an OpenShift Streams for Apache Kafka instance and that the instance is in the &lt;code&gt;Ready&lt;/code&gt; state. Please refer to &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;Getting started with OpenShift Streams for Apache Kafka&lt;/a&gt; for step-by-step instructions to create your Kafka instance.&lt;/p&gt; &lt;h2&gt;Configure OpenShift Streams for Apache Kafka&lt;/h2&gt; &lt;p&gt;Your OpenShift Streams for Apache Kafka instance requires some setup for use with Red Hat OpenShift Connectors. This includes creating &lt;em&gt;Kafka topics&lt;/em&gt; to store messages sent by producers (data sources) and make them available to consumers (data sinks), creating a &lt;em&gt;service account&lt;/em&gt; to allow you to connect and authenticate your connectors with the Kafka instance, and setting up &lt;em&gt;access rules&lt;/em&gt; for the service account to define how your connectors can access and use the associated Kafka instance topics.&lt;/p&gt; &lt;h3&gt;Create a Kafka Topic&lt;/h3&gt; &lt;p&gt;We start with an example Kafka topic, because it's the resource at the center of Kafka operations. Create a topic as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Log in to the hybrid cloud console at &lt;a href="https://console.redhat.com"&gt;console.redhat.com&lt;/a&gt; with your Red Hat account credentials.&lt;/li&gt; &lt;li&gt;Navigate to &lt;strong&gt;Application and Data Services→Streams for Apache Kafka→Kafka instances&lt;/strong&gt;, and select the Kafka instance you created as part of the prerequisites.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;Topics&lt;/strong&gt; tab, and click &lt;strong&gt;Create topic&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Enter a unique name for your topic, for example &lt;strong&gt;test-topic&lt;/strong&gt;. Accept the defaults for partitions, message retention, and replicas.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Create a service account&lt;/h3&gt; &lt;p&gt;Next, you need an account in order to get access to Kafka. Create the account as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the cloud console, select &lt;strong&gt;Service Accounts&lt;/strong&gt; and click &lt;strong&gt;Create Service Account&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Enter a unique name for the service account, and click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Copy the generated &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client Secret&lt;/strong&gt; to a secure location. You'll use these credentials when configuring your connectors.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;I have copied the client ID and secret&lt;/strong&gt; option, and then click &lt;strong&gt;Close&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Set the access level for the service account&lt;/h3&gt; &lt;p&gt;Having an account, you can now obtain the necessary permissions. For this example, you need to be a consumer for one service and a producer for another, so you'll enable both sets of permissions. Set the access levels as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the cloud console, select &lt;strong&gt;Streams for Apache Kafka→Kafka instances&lt;/strong&gt;. Select the Kafka instance you created as part of the prerequisites.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Access&lt;/strong&gt; tab to view the current Access Control List (ACL) for the Kafka instance and then click &lt;strong&gt;Manage access&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;From the &lt;strong&gt;Account&lt;/strong&gt; drop-down menu, select the service account that you created in the previous step, and then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Under &lt;strong&gt;Assign Permissions&lt;/strong&gt;, click &lt;strong&gt;Add permission&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;From the drop-down menu, select &lt;strong&gt;Consume from a topic&lt;/strong&gt;. Set all resource identifiers to &lt;code&gt;is&lt;/code&gt; and all identifier values to &lt;code&gt;*&lt;/code&gt; (an asterisk).&lt;/li&gt; &lt;li&gt;From the drop-down menu, select &lt;strong&gt;Produce to a topic.&lt;/strong&gt; Set all resource identifiers to &lt;code&gt;is&lt;/code&gt; and all identifier values to &lt;code&gt;*&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Figure 1 shows what the access control list should look like.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kafka-acl.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/kafka-acl.png?itok=znBuZtpN" width="1440" height="778" alt="The access control list for your Streams for Apache Kafka instance shows that your service account has access to all topics." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The access control list for your Streams for Apache Kafka instance shows that your service account has access to all topics. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The access control list for your OpenShift Streams for Apache Kafka instance shows that your service account has access to all topics.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Your OpenShift Streams for Apache instance is now configured for use by your connectors.&lt;/p&gt; &lt;h2&gt;Create a source connector&lt;/h2&gt; &lt;p&gt;A source connector consumes events from an external data source and produces Kafka messages. For this getting started guide, you will use the Data Generator source connector. This connector does not actually consume data from an external system, but produces Kafka messages to a topic at a configurable interval. Install the connector as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the cloud console, select &lt;strong&gt;Connectors&lt;/strong&gt; and click &lt;strong&gt;Create a Connectors instance&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select the connector that you want to use as a data source. You can browse through the catalog of available connectors, or search for a particular connector by name and filter the search to look for sink or source connectors. For example, to find the Data Generator source connector, type &lt;strong&gt;data&lt;/strong&gt; in the search box. The list filters to show only the Data Generator&lt;strong&gt; &lt;/strong&gt;connector card, as shown in Figure 2. Click the &lt;strong&gt;Data Generator &lt;/strong&gt;card to select the connector, then click &lt;strong&gt;Next&lt;/strong&gt;. &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/data-generatour-source-connector.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/data-generatour-source-connector.png?itok=87IJPYr_" width="600" height="312" alt="Search for a Connector by name" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Search for a Connector by name. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt;For the &lt;strong&gt;Kafka instance&lt;/strong&gt;, click the card for the OpenShift Streams for Apache Kafka instance that you configured for your connectors, and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Namespace&lt;/strong&gt; page, click &lt;strong&gt;Create preview namespace&lt;/strong&gt; to provision a namespace for hosting the connector instances that you create. This evaluation namespace will remain available for 48 hours. You can create up to four connector instances per namespace. Once the namespace is available, select it and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Configure the core configuration for your connector as follows: &lt;ul&gt; &lt;li&gt; &lt;p&gt;Provide a name for the connector.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Enter the &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client Secret&lt;/strong&gt; of the service account that you created for your connectors, then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Provide connector-specific configuration values. For the Data Generator connector, provide the following information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data shape Format&lt;/strong&gt;: Accept the default, &lt;code&gt;application/octet-stream&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Topic Names&lt;/strong&gt;: Enter the name of the topic that you created previously for your connectors (for example, &lt;strong&gt;test-topic&lt;/strong&gt;).&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Content Type&lt;/strong&gt;: Accept the default, &lt;code&gt;text/plain&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Message&lt;/strong&gt;: Enter the content of the message that you want the connector instance to send to the Kafka topic. For example, type &lt;code&gt;Hello World!&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Period&lt;/strong&gt;: Specify the interval (in milliseconds) at which you want the connector instance to send messages to the Kafka topic. For example, specify &lt;code&gt;10000&lt;/code&gt; to send a message every 10 seconds.&lt;/p&gt; &lt;p&gt;Figure 3 shows an overview of the connector configuration.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/data-generator-source-connector-configuration.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/data-generator-source-connector-configuration.png?itok=kbGu-5kF" width="600" height="419" alt="The Data Generator connector requires specific configuration information." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The Data Generator connector requires specific configuration information. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Configure the error handling policy for your connector instance. The default is &lt;strong&gt;stop&lt;/strong&gt;, which causes the connector to shut down when it encounters an error.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Review the summary of the configuration properties and click &lt;strong&gt;Create Connector&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your connector instance should now be listed in the table of connectors. After a couple of seconds, the status of your connector instance will change to the &lt;code&gt;Ready&lt;/code&gt; state, and it will start producing messages and sending them to its associated Kafka topic.&lt;/p&gt; &lt;p&gt;From the connectors table, you can stop, start, and delete your connector, as well as edit its configuration, by clicking the options icon (three vertical dots).&lt;/p&gt; &lt;h2&gt;Create a sink connector&lt;/h2&gt; &lt;p&gt;A sink connector consumes messages from a Kafka topic and sends them to an external system. For this example, use the HTTP Sink connector, which consumes Kafka messages from one or more topics and sends the messages to an HTTP endpoint.&lt;/p&gt; &lt;p&gt;The Webhook.site service offers a convenient way to obtain a general-purpose HTTP endpoint. Open a new tab in your browser and navigate to &lt;a href="https://webhook.site"&gt;https://webhook.site&lt;/a&gt;. The page displays a unique URL that you can use as a data sink, as shown in Figure 4.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/webhook-site.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/webhook-site.png?itok=kTROqrN-" width="1440" height="671" alt="Copy the long, unique URL generated for you by Webhook.site." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Copy the long, unique URL generated for you by Webhook.site. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Once you have that URL, configure the endpoint as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In the cloud console, select &lt;strong&gt;Connectors&lt;/strong&gt; and click &lt;strong&gt;Create Connectors instance&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;To find the &lt;strong&gt;HTTP Sink&lt;/strong&gt; connector, enter &lt;strong&gt;http&lt;/strong&gt; in the search field. Click the &lt;strong&gt;HTTP Sink&lt;/strong&gt; connector card and then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select the OpenShift Streams for Apache Kafka instance for the connector to work with.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Namespace&lt;/strong&gt; page, click the &lt;strong&gt;eval&lt;/strong&gt; namespace that you created when you created the source connector. Then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Configure the core configuration for your connector: &lt;ul&gt; &lt;li&gt;Provide a unique name for the connector.&lt;/li&gt; &lt;li&gt;Type the &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client Secret&lt;/strong&gt; of the service account that you created for your connectors, and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Provide the connector-specific configuration for your connector. For the &lt;strong&gt;HTTP sink connector&lt;/strong&gt;, provide the following information: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Data shape Format&lt;/strong&gt;: Accept the default, &lt;code&gt;application/octet-stream&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Method&lt;/strong&gt;: Accept the default, &lt;code&gt;POST&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;URL&lt;/strong&gt;: Type your unique URL from Webhook.site.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Topic Names&lt;/strong&gt;: Type the name of the topic that you used for the source connector.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Set the error handling policy to &lt;strong&gt;stop&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Review the summary of the configuration properties and click &lt;strong&gt;Create Connector&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your connector instance will be added to the table of connectors. After a couple of seconds, the status of your connector instance will change to the &lt;code&gt;Ready&lt;/code&gt; state. The connector consumes messages from the associated Kafka topic and sends them to the HTTP endpoint.&lt;/p&gt; &lt;p&gt;Visit the unique URL you got from Webhook.site in your browser to see the HTTP POST calls with the &lt;code&gt;"Hello World!"&lt;/code&gt; messages that you defined in the source connector, as shown in Figure 5.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/webhook-site-posts.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/webhook-site-posts.png?itok=tyQ_TpCB" width="1440" height="500" alt="Your Webhook.site URL displays the HTTP POST messages received." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Your Webhook.site URL displays the HTTP POST messages received. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;Congratulations—you have created your first Red Hat OpenShift Connector instances. From here, you can create other source and sink connectors to a variety of external systems and cloud services.&lt;/p&gt; &lt;p&gt;Stay tuned for the next article in this series, in which we will show you how to use Red Hat OpenShift Connectors to connect to a cloud-based database and capture data change events from that database.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/09/get-started-red-hat-openshift-connectors" title="Get started with Red Hat OpenShift Connectors"&gt;Get started with Red Hat OpenShift Connectors&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bernard Tison</dc:creator><dc:date>2022-06-09T10:45:00Z</dc:date></entry><entry><title>Detecting nondeterministic test cases with Bunsen</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/09/detecting-nondeterministic-test-cases-bunsen" /><author><name>Serhei Makarov</name></author><id>29f395a8-179b-4628-8ea9-4e837ac160f1</id><updated>2022-06-09T07:00:00Z</updated><published>2022-06-09T07:00:00Z</published><summary type="html">&lt;p&gt;Many open source projects have test suites that include nondeterministic test cases with unpredictable behavior. Tests might be nondeterministic because they launch several parallel processes or threads that interact in an unpredictable manner, or because they depend on some activity in the operating system that has nondeterministic behavior. The presence of these tests can interfere with automated regression checking in &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD pipelines&lt;/a&gt;. This article shows how to automate the discovery of nondeterministic test cases using a short Python script based on the &lt;a href="https://sourceware.org/git/?p=bunsen.git;a=summary"&gt; Bunsen&lt;/a&gt; test suite analysis toolkit.&lt;/p&gt; &lt;h2&gt;The problem: Finding nondeterministic ("flaky") test cases&lt;/h2&gt; &lt;p&gt;Test cases in an open source project's test suite can have nondeterministic behavior and produce different outcomes when run repeatedly. Such test cases are commonly referred to as &lt;em&gt;flaky,&lt;/em&gt; and their presence in a test suite tends to complicate the evaluation of test results. Without additional investigation, a PASS or FAIL outcome for a nondeterministic test case doesn't conclusively prove the presence or absence of a problem.&lt;/p&gt; &lt;p&gt;Nondeterministic test cases are usually found in test suites of projects such as SystemTap and the GNU Debugger (GDB) because they provide value when testing the project's functionality under ideal conditions. Rewriting these test suites to eliminate nondeterminism would be a large and low-priority task tying up a large amount of scarce developer time. Therefore, it's worthwhile to develop tools to analyze test results from a project and identify nondeterministic test cases. A developer reading test results could use this analysis to recognize nondeterministic test cases and interpret their outcomes separately from outcomes of reliable test cases.&lt;/p&gt; &lt;p&gt;In a previous article,&lt;a href="https://developers.redhat.com/blog/2021/05/10/automating-the-testing-process-for-systemtap-part-2-test-result-analysis-with-bunsen"&gt; Automating the testing process for SystemTap, Part 2: Test result analysis with Bunsen&lt;/a&gt;, I described Bunsen, a tool that collects a set of test result log files from a project and stores them in a deduplicated Git repository together with an index in JSON format. Bunsen also provides a Python library for accessing the data in this repository. These capabilities can be used to implement a script to detect nondeterministic test cases.&lt;/p&gt; &lt;h2&gt;Developing the script&lt;/h2&gt; &lt;p&gt;The overall strategy of the script is to find test cases that have been run multiple times on the same system configuration with varying outcomes. Such test cases are likely to be nondeterministic.&lt;/p&gt; &lt;h3&gt;Basic setup&lt;/h3&gt; &lt;p&gt;The analysis script starts by importing and initializing the Bunsen library:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;1 #!/usr/bin/env python3 2 info="""Detect nondeterministic testcases that yield different outcomes when tested 3 multiple times on the same configuration.""" 4 from bunsen import Bunsen, BunsenOptions 5 if __name__=='__main__': 6 BunsenOptions.add_option('source_repo', group='source_repo', 7 cmdline='source-repo', default=None, 8 help_str="Use project commit history from Git repo &lt;path&gt;", 9 help_cookie="&lt;path&gt;") 10 BunsenOptions.add_option('branch', group='source_repo', default=None, 11 help_str="Use project commit history from &lt;branch&gt; in source_repo", 12 help_cookie="&lt;branch&gt;") 13 BunsenOptions.add_option('project', group='filtering', default=None, 14 help_str="Restrict the analysis to testruns in &lt;projects&gt;", 15 help_cookie="&lt;projects&gt;") 16 17 import git 18 import tqdm 19 from common.utils import * # add_list, add_set 20 if __name__=='__main__': 21 22 b, opts = Bunsen.from_cmdline(info=info) 23 projects = opts.get_list('project', default=b.projects) 24 repo = git.Repo(opts.source_repo) for # Iterate all testruns in projects # Collect information from the testrun # Print the results&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A Bunsen analysis script is a Python program that imports the &lt;code&gt;bunsen&lt;/code&gt; module. Lines 5-15 of the preceding script define the following options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;source_repo&lt;/code&gt; identifies a Git repository containing up-to-date source code for the project. The commit history of this repository identifies the relative version order of test runs.&lt;/li&gt; &lt;li&gt;&lt;code&gt;branch&lt;/code&gt; identifies a branch within &lt;code&gt;source_repo&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;project&lt;/code&gt; names a project within the Bunsen repository, and is present because the Bunsen repository can store test results from more than one project. Test results from separate projects are stored in separate branches, and the analysis script can be instructed to scan and compare test results from a single project or from a subset of the projects. If this option is omitted, all test runs in the Bunsen repository will be scanned.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Options for an analysis script can be passed as command-line arguments or specified in the Bunsen repository's configuration file. For example, if the Bunsen repository is stored under &lt;code&gt;/path/to/bunsen/.bunsen&lt;/code&gt;, the configuration file is located at &lt;code&gt;/path/to/bunsen/.bunsen/config&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The second part of the script (lines 20-24) instantiates the following objects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;b&lt;/code&gt;, an instance of the &lt;a href="https://sourceware.org/git/?p=bunsen.git;a=blob;f=bunsen/repo.py;h=fa6868d9380523891ddac848a2e4651cd5f0dfa1;hb=3fce15d750f78e0f2c54040638aa4e57b2cc3803#l309"&gt;Bunsen&lt;/a&gt; class providing access to the Bunsen repository&lt;/li&gt; &lt;li&gt;&lt;code&gt;opts&lt;/code&gt;, an instance of the &lt;a href="https://sourceware.org/git/?p=bunsen.git;a=blob;f=bunsen/repo.py;h=fa6868d9380523891ddac848a2e4651cd5f0dfa1;hb=3fce15d750f78e0f2c54040638aa4e57b2cc3803#l1811"&gt;BunsenOptions&lt;/a&gt; class providing access to the script's options&lt;/li&gt; &lt;li&gt;&lt;code&gt;repo&lt;/code&gt;, an instance of the &lt;code&gt;git.Repo&lt;/code&gt; class from the &lt;a href="https://gitpython.readthedocs.io/en/stable/"&gt;GitPython library&lt;/a&gt;, providing access to the version history of the project in the &lt;code&gt;source_repo&lt;/code&gt; repository.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Collecting the test results&lt;/h3&gt; &lt;p&gt;A test case is considered to be nondeterministic if it was tested more than once on the same &lt;code&gt;source_repo&lt;/code&gt; commit and system configuration with varying outcomes—a PASS outcome in one test run and a FAIL outcome in another, for instance. To determine which test cases produce varying outcomes, the script gathers a list of test runs for each commit and configuration. The script then iterates through the test runs for each combination and compares the outcomes of each test case for different test runs. The script uses a dictionary named &lt;code&gt;all_testruns&lt;/code&gt; to store the list of test runs corresponding to each commit and configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;26 all_testruns = {} # maps (commit, config) -&gt; list(Testrun) 27 28 for testrun in b.testruns(opts.projects): 29 commit, config = testrun.get_source_commit(), testrun.get_config() 30 if commit is None: continue 31 add_list(all_testruns, (commit,config), testrun) for # Iterate all (commit, config) # Iterate the set of testruns matching (commit, config), # and compare the outcome of each testcase to detect nondeterminism # Print the results&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;An instance of the &lt;a href="https://sourceware.org/git/?p=bunsen.git;a=blob;f=bunsen/model.py;h=e9ccea97b4cacb2b6b42f6e79de55b7a2a077fa5;hb=3fce15d750f78e0f2c54040638aa4e57b2cc3803#l1070"&gt;Testrun&lt;/a&gt; class in the Bunsen library represents a single test run. The instance provides access to the commit that was tested, the system configuration, and the outcomes of individual test cases. The &lt;code&gt;all_testruns&lt;/code&gt; dictionary, defined on line 26, maps a (commit, config) pair to a list of &lt;code&gt;Testrun&lt;/code&gt; instances.&lt;/p&gt; &lt;p&gt;For each test run, the loop invokes the utility method &lt;code&gt;add_list&lt;/code&gt; at line 31 to add the test run to the dictionary. The &lt;code&gt;add_list&lt;/code&gt; method is a simple utility method that appends a value to a list stored at a specified key:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;def add_list(d,k,v): if k not in d: d[k] = [] d[k].append(v)&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Identifying the nondeterministic test cases&lt;/h3&gt; &lt;p&gt;Next, the script iterates over the list of &lt;code&gt;Testrun&lt;/code&gt; objects for each commit and configuration. To record the list of test cases that produced varying outcomes, the script uses a second dictionary named &lt;code&gt;known_flakes&lt;/code&gt;, whose keys are (testcase, config) pairs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;26 all_testruns = {} # maps (commit, config) -&gt; list(Testrun) 27 28 for testrun in b.testruns(opts.projects): 29 commit, config = testrun.get_source_commit(), testrun.get_config() 30 if commit is None: continue 31 add_list(all_testruns, (commit,config), testrun) 32 33 known_flakes = {} # maps (tc_info, config) -&gt; set(commit) 34 # where tc_info is (name, subtest, outcome) 35 36 for commit, config in tqdm.tqdm(all_testruns, \ 37 desc="Scanning configurations", unit="configs"): if len(all_testruns[commit, config]) &lt;= 1: continue # no possibility of flakes commit_testcases = {} # maps tc_info -&gt; list(Testrun) for testrun in all_testruns[commit, config]: # Gather the list of failing tc_info tuples appearing in testrun for # each tc_info tuple that appears in some testrun": # Check whether the failing tuple appears in all testruns; # If not, mark the tuple as a flake # Print the results&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The second loop, iterating over the commits and configurations, could take a long time. So the script uses the Python &lt;a href="https://tqdm.github.io/"&gt;tqdm&lt;/a&gt; library to display a progress bar (lines 36-37).&lt;/p&gt; &lt;p&gt;After the remaining code is filled in, the second loop appears as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;… 36 for commit, config in tqdm.tqdm(all_testruns, \ 37 desc="Scanning configurations", unit="configs"): 38 39 if len(all_testruns[commit, config]) &lt;= 1: 40 continue # no possibility of flakes 41 42 commit_testcases = {} # maps tc_info -&gt; list(Testrun) 43 44 for testrun in all_testruns[commit, config]: 45 for tc in testrun.testcases: 46 if tc.is_pass(): continue 47 tc_info = (tc.name, tc.outcome, tc.subtest) 48 add_list(commit_testcases, tc_info, testrun) 49 50 expected_testruns = len(all_testruns[commit, config]) 51 for tc_info in commit_testcases: 52 if len(commit_testcases[tc_info]) &lt; n_testruns: 53 # XXX tc_info didn't appear in all runs 54 add_set(known_flakes, tc_info, commit) …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The second loop skips (commit, config) pairs for which only one test run was found (lines 39-40). For each of the other test runs, the loop iterates over its test case outcomes and gathers a list of test failures that appear in the test run. Test case outcomes are represented by instances of Bunsen's &lt;code&gt;Testcase&lt;/code&gt; class. In accordance with the &lt;a href="https://ftp.gnu.org/old-gnu/Manuals/dejagnu-1.3/html_mono/dejagnu.html"&gt;DejaGNU framework's test result model&lt;/a&gt;, a &lt;code&gt;Testcase&lt;/code&gt; object has fields called 'name' (the name of the top-level Expect file defining the test case), 'outcome' (one of the &lt;a href="https://ftp.gnu.org/old-gnu/Manuals/dejagnu-1.3/html_mono/dejagnu.html#SEC6"&gt;standard POSIX outcome codes&lt;/a&gt;, such as &lt;code&gt;PASS&lt;/code&gt;, &lt;code&gt;FAIL&lt;/code&gt;, or &lt;code&gt;UNTESTED&lt;/code&gt;), and 'subtest' (a string providing additional information about the outcome).&lt;/p&gt; &lt;p&gt;A third dictionary named &lt;code&gt;commit_testcases&lt;/code&gt; stores failing test case outcomes. The dictionary maps the (name, outcome, subtest) tuple describing the test failure to a list of test runs where this tuple was found to occur. The script assembles &lt;code&gt;commit_testcases&lt;/code&gt; on lines 44-48 and iterates over it on lines 51-54 to collect every (name, outcome, subtest) tuple that appeared in some test runs but not all of them. Such a tuple fits our definition of a varying test outcome and therefore is stored in the &lt;code&gt;known_flakes&lt;/code&gt; dictionary. The &lt;code&gt;known_flakes&lt;/code&gt; dictionary maps each (testcase, config) combination to a set of commit IDs on which that combination was found to produce varying outcomes.&lt;/p&gt; &lt;h3&gt;Reporting the nondeterministic test cases&lt;/h3&gt; &lt;p&gt;Having accumulated a list of suspected nondeterministic tests in the &lt;code&gt;known_flakes&lt;/code&gt; dictionary, the script iterates through it and prints the nondeterministic tests' outcomes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;56 sorted_tc = [] 57 for tc_info in all_testcases: 58 sorted_tc.append((tc_info, all_testcases[tc_info])) 59 sorted_tc.sort(reverse=True, key=lambda tup: len(tup[1])) 60 for tc_info, commits in sorted_tc: 61 print(len(commits),"commits have nondeterministic",tc_info)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The script sorts the test outcomes (on lines 56-59) in decreasing order of frequency: test cases found to produce varying outcomes on a larger number of commits are printed first. An additional loop can be added to print the commits on which the test outcomes were found to be nondeterministic:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;60 for tc_info, commits in sorted_tc: 61 print(len(commits),"commits have nondeterministic",tc_info) 62 for hexsha in commits: 63 commit = repo.commit(hexsha) 64 print("*",commit.hexsha[:7],commit.summary)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Lines 63-64 use the GitPython library and the &lt;code&gt;git.Repo&lt;/code&gt; object that was instantiated at the beginning of the script to retrieve a summary of the commit message.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://sourceware.org/git/?p=bunsen.git;a=blob;f=scripts-main/find_flakes.py;h=debc60f7b00718c1fe7d01e97afe9689e9d957dd;hb=HEAD"&gt;completed analysis script&lt;/a&gt; is less than 100 lines of Python code. When tested on a modest laptop (2.3GHz i3-6100U), the script took approximately 42 seconds with a maximum resident memory size of 285MB to scan a Bunsen repository from the SystemTap project containing data from 4,158 test runs across 693 commits. Within that Bunsen repository, 368 (commit, config) pairs were tested by more than one test run and provided useful data for the analysis script. In practice, more complex analysis scripts that compare test case results over time (rather than within the same commit) will tend to have larger RAM requirements.&lt;/p&gt; &lt;p&gt;When run, the analysis script produces output similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;72 commits have nondeterministic ('systemtap.base/attach_detach.exp', 'FAIL: attach_detach (initial load) - EOF\n', 'FAIL') 72 commits have nondeterministic ('systemtap.base/attach_detach.exp', 'FAIL: attach_detach (initial load) - begin seen (1 0)\n', 'FAIL') 61 commits have nondeterministic ('systemtap.examples/check.exp', 'FAIL: systemtap.examples/io/ioblktime run\n', 'FAIL') 51 commits have nondeterministic ('systemtap.base/utrace_p5.exp', 'FAIL: UTRACE_P5_07 unexpected output (after passing output)\n', 'FAIL') 47 commits have nondeterministic ('systemtap.syscall/tp_syscall.exp', 'FAIL: 32-bit madvise tp_syscall\n', 'FAIL') 40 commits have nondeterministic ('systemtap.base/abort.exp', 'FAIL: abort: TEST 6: abort() in timer.profile (using globals): stdout: string should be "fire 3!\\nfire 2!\\nfire 1!\\n", but got "fire 2!\n', 'FAIL') 39 commits have nondeterministic ('systemtap.syscall/tp_syscall.exp', 'FAIL: 64-bit clock tp_syscall\n', 'FAIL') 39 commits have nondeterministic ('systemtap.syscall/tp_syscall.exp', 'FAIL: 32-bit clock tp_syscall\n', 'FAIL') 38 commits have nondeterministic ('systemtap.syscall/tp_syscall.exp', 'FAIL: 32-bit socket tp_syscall\n', 'FAIL') 37 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_start_disabled_iter_5 (invalid output)\n', 'FAIL') 37 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_timer_50ms (invalid output)\n', 'FAIL') 36 commits have nondeterministic ('systemtap.syscall/tp_syscall.exp', 'FAIL: 64-bit madvise tp_syscall\n', 'FAIL') 34 commits have nondeterministic ('systemtap.bpf/nonbpf.exp', 'FAIL: bigmap1.stp unexpected output\n', 'FAIL') 33 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_timer_10ms (invalid output)\n', 'FAIL') 33 commits have nondeterministic ('systemtap.bpf/bpf.exp', 'FAIL: timer2.stp incorrect result\n', 'FAIL') 33 commits have nondeterministic ('systemtap.bpf/bpf.exp', 'KFAIL: bigmap1.stp unexpected output (PRMS: BPF)\n', 'KFAIL') 33 commits have nondeterministic ('systemtap.bpf/bpf.exp', 'FAIL: stat3.stp incorrect result\n', 'FAIL') 33 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_timer_100ms (invalid output)\n', 'FAIL') 32 commits have nondeterministic ('systemtap.server/client.exp', 'FAIL: New trusted servers matches after reinstatement by ip address\n', 'FAIL') 32 commits have nondeterministic ('systemtap.unprivileged/unprivileged_myproc.exp', 'FAIL: unprivileged myproc: --unprivileged process.thread.end\n', 'FAIL') 32 commits have nondeterministic ('systemtap.base/procfs_bpf.exp', 'FAIL: PROCFS_BPF initial value: cat: /var/tmp/systemtap-root/PROCFS_BPF/command: No such file or directory\n', 'FAIL') 32 commits have nondeterministic ('systemtap.base/abort.exp', 'FAIL: abort: TEST 6: abort() in timer.profile (using globals): stdout: string should be "fire 3!\\nfire 2!\\nfire 1!\\n", but got "fire 3!\n', 'FAIL') 31 commits have nondeterministic ('systemtap.syscall/nd_syscall.exp', 'FAIL: 32-bit clock nd_syscall\n', 'FAIL') 31 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_start_enabled_iter_4 (invalid output)\n', 'FAIL') 31 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_start_enabled_iter_5 (invalid output)\n', 'FAIL') 31 commits have nondeterministic ('systemtap.onthefly/kprobes_onthefly.exp', 'FAIL: kprobes_onthefly - otf_start_disabled_iter_3 (invalid output)\n', 'FAIL') 30 commits have nondeterministic ('systemtap.syscall/syscall.exp', 'FAIL: 32-bit clock syscall\n', 'FAIL')&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article illustrates how Bunsen's Python library can be used to quickly develop analysis scripts to answer questions about a project's testing history. More generally, the example demonstrates the benefit of keeping a long-term archive of test results that can be used to answer questions about a project's testing history.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/09/detecting-nondeterministic-test-cases-bunsen" title="Detecting nondeterministic test cases with Bunsen"&gt;Detecting nondeterministic test cases with Bunsen&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Serhei Makarov</dc:creator><dc:date>2022-06-09T07:00:00Z</dc:date></entry><entry><title type="html">Vlog: WildFly gRPC</title><link rel="alternate" href="https://www.youtube.com/watch?v=UYSNM9Dy5M4" /><author><name>Harald Pehl</name></author><id>https://www.youtube.com/watch?v=UYSNM9Dy5M4</id><updated>2022-06-09T00:00:00Z</updated><dc:creator>Harald Pehl</dc:creator></entry></feed>
